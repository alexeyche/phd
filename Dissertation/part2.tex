
\chapter{Регрессионный анализ многомерных временных рядов на основе динамической спайковой модели} \label{chapt2}

\section{Формальная постановка задачи} \label{sect2_1}
\indent Рассмотрим наблюдаемый объект преобразующий входное воздействие $X(t) \in \mathbb{R}^{n_{x}}$ в выходное $Y(t) \in \mathbb{R}^{n_{y}}$. Формально можно представить это преобразование некоторым оператором $f_{0}$:
\begin{equation}
Y(t) = f_{0}(X(t), h),
\end{equation}
здесь выходной сигнал $Y(t)$ зависит от входного воздействия, $X(t)$ и набора параметров $h$ которые недоступны непосредственному наблюдению.
\indent Смоделируем описанную зависимость оператором $f$ преобразующим входное воздействие $X(t)$ в моделируемое выходное $Y_{M}(t)$:
\begin{equation}
Y_{M}(t) = f(X(t), \theta),
\end{equation}
где $\theta$ -- набор параметров который определяется на основе алгоритмов описанных в работе.\\
\indent Для оценки соответсвия модели объекту введем квадратичную функцию невязки 
\begin{equation} \label{eq:p2_1_cost}
C_{int}(Y, Y_{M}, \theta) = \int_{0}^{T} (Y(t) - Y_{M}(t, \theta))^2 dt 
\end{equation}
где $T$ -- время жизни наблюдаемого объекта.

\section{Модель динамической спайковой нейронной сети} \label{sect2_2}
\subsection{Однослойная модель}
\indent В качестве базовой модели для решения поставленной задачи рассмотрим пороговый интегратор (\textit{Integrate and fire} \cite{burkitt2006review}). Уравнение динамики задается ДУ
\begin{equation}
\tau_{mem} \frac{dU}{dt} = -U(t) + W I(t),
\end{equation}
где $U(t) \in \mathbb{R}^{n}$, мембрана нейронной популяции размера $n$, $I(t) \in \mathbb{R}^{m}$ приложенный ток, рассматриваемый как входной сигнал приложенный к системе с целью получить отклик $U(t)$, $W$ -- матрица $n \times m$, обозначающая веса нейронной популяции, которые формируют линейное преобразование приложенного тока $I(t)$. Параметр $\tau_{mem}$ обозначает временную константу интегратора ($RC$ параметр апериодического звена \cite{auto_control_theory}). Отклик нейронной популяции $U(t)$ проходит нелинейное преобразование функцией активации
\begin{equation} \label{eq:p2_2_act}
A(t) = F_{act}(U(t)),
\end{equation}
здесь $F_{act}$ -- функция активации, $A(t)$ обозначает интенсивность негомогенного Пуассоновского процесса, реализация которого характеризует появления событий-спайков в нейронной популяции, которые можно представить в виде импульсной формы сигнала, или сумм дельта-функций
\begin{equation} \label{eq:p2_2_poiss}
S(t) = \sum_{i=1}^n \sum_{T_{i}} \delta(t - t_{j}^{f_{i}}).
\end{equation}
где $T_{i} = {t_{1}^{f_{i}}, t_{2}^{f_{i}}, \dots, t_{k}^{f_{i}}}$ множество времен всех произведенных спайков $i$-ым нейроном из популяции.\\
\indent Рассматривая приложенный ток $I(t)$ как вход $X(t)$ и моделируемый сигнал $S(t)$, как целевой $Y(t)$, решение задачи регрессии сводится к задаче оптимизации параметров модели $W$:
\begin{equation}
\min_{W \in \mathbb{R}^{m x n}} C_{int}(W) = C_{int}(W^{\ast}),
\end{equation}
где $W^{\ast}$ -- оптимальный набор параметров.

\subsection{Многослойная модель}
\indent Аналогично многослойному персептрону, можно произвести машстабирование модели на несколько слоёв. В модели из $l$ слоёв, $i$-ый слой можно описать ДУ
\begin{equation} \label{eq:p2_2_mlayer}
\tau_{mem} \frac{dU_{i}}{dt} = -U_{i}(t) + W_{i} I_{i}(t),\quad i \in [1, l],
\end{equation}
здесь $U_{i}$ -- мембрана нейронной популяции слоя $i$. Для первого слоя $I_{1}(t)$ рассматривается как вход $X(t)$, а для слоя $i \in [2, l]$ приложенный ток выражается сглаженной активностью предыдушего слоя $I_{i}(t) = \hat S_{i-1}(t)$. Сглаживание произведено низкочастотным синаптическим фильтром
\begin{equation} \label{eq:p2_2_syn}
\hat S_{i}(t) = \int_{0}^{T} S_{i}(t') exp\Big(-\frac{t-t'}{\tau_{syn}}\Big)dt' = \sum_{f_{i}} exp\Big(-\frac{t-t^{f_{i}}}{\tau_{syn}}\Big),
\end{equation}
здесь $S_{i}(t)$ реализация процесса $i$-ого слоя популяции, $\tau_{syn}$ синаптическая константа. Каждый слой задается размером популяции и матрицы $W_{1}, W_{2}, \dots, W_{l}$ имеют соответствующие им размерности.

\section{Обучение модели} \label{sect2_3}
\subsection{Модификация метода обратного распространение ошибки}
\iffalse
\indent Ввиду наличие нелинейности в \eqref{eq:p2_2_act} и нарушение непрерывности в \eqref{eq:p2_2_poiss} оптимизация параметров подобных моделей всегда являлась довольно трудоемкой задачей. В некоторых работах в которых рассматривается правило обучения для однослойных спайковых нейронных сетей \cite{mohemmed2012span, florian2008tempotron, ponulak2005resume}. основным способом преодолеть проблемы связанные с нарушением непрерывности было введение фильтрации интересующих спайковых сигналов при помощи низкочастного фильтра, что ко всему прочему, судя по работам связанным с тематикой  \textit{STDP} имеет довольно непосредственное отношение к биологическому механизму синаптической пластичности \cite{dan2004spike}.\\
\fi
\indent Модель описсанная в \eqref{eq:p2_2_mlayer} несет в себе нелинейность в виде функции активации \eqref{eq:p2_2_act}, также реализация активности нейрона в виде Пуассоновского процесса несет в себе разрывную функцию, в итоге прямую формулировку частной производной относительно параметров модели вывести невозможно. Однако, усреднив активость нейрона низкочастотным фильтром информацию о градиенте можно будет получить, и передать другим слоям нейронной сети.\\
\indent Таким образом, для квадратичной функции невязки \eqref{eq:p2_1_cost} можно определить приближенное онлайн вычисление
\begin{equation}
C(t) = \int_{0}^{T} (Y(t-t') - Y_{M}(t-t', \theta))^2 f_{l}(t') dt'
\end{equation} 
используя обработку низкочастотным фильтром
\begin{equation} \label{eq:p2_3_filter}
f_{l}(t) = exp\Big(-\frac{t}{\tau_{l}}\Big),
\end{equation}
где $\tau_{l}$ -- параметр фильтра.\\
Посчитанная онлайн функция невзяки позволяет найти частную производную для параметров последнего слоя (классическое дельта-правило \cite{jain1998fusion})
\begin{equation}
\Delta W_{l} = \frac{\partial C(t)}{\partial u_{l}(t)} = - \space \hat S_{l-1}(t) \odot(\hat S_{l}(t) - y(t)),
\end{equation}
где $\odot$ -- внешнее произведение, $\hat S_{l}(t)$ активность слоя $l$, обработанная синаптическим фильтром \eqref{eq:p2_2_syn}.\\
\indent Для скрытых слоев можно применить логику аналогичную правилу обратного распространения ошибки \cite{wossermanNeuroComp}:
\begin{equation} \label{eq:p2_3_bp}
\delta u_{i}(t) = \frac{\partial C(t)}{\partial u_{i}(t)} = (W_{i+1} \delta u_{i+1}) \otimes F_{act}'(u_{i}(t))
\end{equation}
\begin{equation}
\Delta W_{i} = - \delta u_{i}(t) \odot I_{i}(t),
\end{equation}
здесь $\otimes$ означает поэлементное умножение.
\subsection{Регулируемая обратная связь}
\iffalse
\indent Не смотря на то, что на сегодняшний день правило обратного распространения ошибки до сих является наиболее популярным и наиболее эффективным методом обучения нейронных сетей, биологическая оправданность этого способа стоит под вопросом не менее давно \cite{rumelhart1986david, grossberg1987competitive, crick1989recent}.\\
\indent Серия работ на тематику регулируемой обратной связи (\textit{feedback alignment}) \cite{lillicrap2014random, nokland2016direct, liao2015important} открывает новые направления работ на тематику методов обучения равных по эффективности методу обратному распространению ошибки, но исключающие наиболее ненатуральный момент в нём -- необходимость обратного распространения информации в сети.\\
\fi
\indent Как видно из результатов \cite{lillicrap2014random} для работы правило обучения, достаточно заменить матрицу $W_{i+1}$ из \eqref{eq:p2_3_bp} на случайную матрицу $B_{i}$, при этом исследование  \cite{nokland2016direct} показало, что и второй множитель этой матрицы можно заменить, на уровень ошибки $C(t)$, получая таким образом
\begin{equation} \label{eq:p2_3_fa}
\delta u_{i}(t) = \frac{\partial C(t)}{\partial u_{t}(t)} = (B_{i} C(t)) \otimes F_{act}'(u_{i}(t)).
\end{equation}
 
