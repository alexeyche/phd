
\chapter{Регрессионный анализ многомерных временных рядов на основе динамической спайковой модели} \label{chapt2}

\section{Формальная постановка задачи} \label{sect2_1}
\indent Рассмотрим пару многомерных временных рядов $x(t) \in \mathbb{R}^{n_{x}}, y(t) \in \mathbb{R}^{n_{y}}$, где $n_{x}, n_{y}$ размерности $x(t), y(t)$, соответственно. Смоделировать временной ряд $y(t)$ на основе ряда $x(t)$, означает найти  такую динамическую систему 
\begin{equation}
\frac{dy'}{dt} = F(y'(t), x(t)),
\end{equation}
что для данного начального условия $y'_{0}$ интегральная кривая $y'(t)$ динамической системы удовлетворяет условию
\begin{equation}
C_{int} = \int_{0}^{T} C(y(t), y'(t)) dt \rightarrow 0,
\end{equation}
где $C(y'(t), y(t))$ функция ошибки обозначающая степень сходимости поиска модели. Поставим задачу моделирования временного ряда $y(t)$ на основе временного ряда $x(t)$ как задачу регресионного анализа.


\section{Модель динамической спайковой нейронной сети} \label{sect2_2}
\subsection{Однослойная модель}
\indent В качестве базовой модели для решения поставленной задачи рассмотрим пороговый интегратор (\textit{Integrate and fire} \cite{burkitt2006review}). Уравнение динамики задается ДУ
\begin{equation}
\tau_{mem} \frac{du}{dt} = -u(t) + W I(t),
\end{equation}
где $u(t) \in \mathbb{R}^{n}$, мембрана нейронной популяции размера $n$, $I(t) \in \mathbb{R}^{m}$ приложенный ток, рассматриваемый как входной сигнал приложенный к системе с целью получить отклик $u(t)$, $W$ -- матрица $n \times m$, обозначающая веса нейронной популяции, которые формируют линейное преобразование приложенного тока $I(t)$. Параметр $\tau_{mem}$ обозначает временную константу интегратора ($RC$ параметр апериодического звена \cite{auto_control_theory}). Отклик нейронной популяции $u(t)$ проходит нелинейное преобразование функцией активации
\begin{equation} \label{eq:p2_2_act}
A(t) = F_{act}(u(t)),
\end{equation}
здесь $F_{act}$ -- функция активации, $A(t)$ обозначает частоту негомогенного Пуассоновского процесса, реализация которого характеризует появления событий-спайков в нейронной популяции, которые можно представить в виде импульсной формы сигнала, или сумм дельта-функций
\begin{equation} \label{eq:p2_2_poiss}
S(t) = \sum_{i=1}^n \sum_{f_{i}} \delta(t - t_{j}^{f_{i}}).
\end{equation}
где $f_{i} = {t_{1}^{f_{i}}, t_{2}^{f_{i}}, \dots, t_{k}^{f_{i}}}$ множество времен всех произведенных спайков $i$-ым нейроном из популяции.\\
\indent Рассматривая приложенный ток $I(t)$ как вход $x(t)$ и моделируемый сигнал $S(t)$, как целевой $y(t)$, решение задачи регрессии сводится к задаче оптимизации параметров модели $W$:
\begin{equation}
\min_{W} C_{int}.
\end{equation}
\subsection{Многослойная модель}
\indent Аналогично многослойному персептрону, можно произвести машстабирование модели на несколько слоёв. В модели из $l$ слоёв, $i$-ый слой можно описать ДУ
\begin{equation}
\tau_{mem} \frac{du_{i}}{dt} = -u_{i}(t) + W_{i} I_{i}(t),\quad i \in [0, l],
\end{equation}
где для первого слоя $I_{1}(t)$ рассматривается как вход $x(t)$, а для слоя $i \in [2, l]$ приложенный ток выражается сглаженной активностью предыдушего слоя $I_{i}(t) = S_{i-1}^{s}(t)$. Сглаживание произведено низкочастотным синаптическим фильтром
\begin{equation} \label{eq:p2_2_syn}
S_{i}^{s}(t) = \int_{0}^{T} S_{i}(t') exp\Big(-\frac{t-t'}{\tau_{syn}}\Big)dt' = \sum_{f_{i}} exp\Big(-\frac{t-t^{f_{i}}}{\tau_{syn}}\Big),
\end{equation}
здесь $\tau_{syn}$ синаптическая константа. Каждый слой задается размером популяции и матрицы $W_{1}, W_{2}, \dots, W_{l}$ имеют соответствующие им размерности.

\section{Обучение модели} \label{sect2_3}
\subsection{Обратное распространение ошибки}
\indent Ввиду наличие нелинейности в \eqref{eq:p2_2_act} и нарушение непрерывности в \eqref{eq:p2_2_poiss} оптимизация параметров подобных моделей всегда являлась довольно трудоемкой задачей. В некоторых работах в которых рассматривается правило обучения для однослойных спайковых нейронных сетей \cite{mohemmed2012span, florian2008tempotron, ponulak2005resume}. основным способом преодолеть проблемы связанные с нарушением непрерывности было введение фильтрации интересующих спайковых сигналов при помощи низкочастного фильтра, что ко всему прочему, судя по работам связанным с тематикой  \textit{STDP} имеет довольно непосредственное отношение к биологическому механизму синаптической пластичности \cite{dan2004spike}.\\
\indent В случае квадратичной функции ошибки для матрицы последнего слоя правило обучения можно свести к классическому дельта-правилу \cite{jain1998fusion}
\begin{equation}
\Delta W_{l} = - \space I_{l-1}(t) \otimes (\hat S_{l}(t) - y(t)),
\end{equation}
где $\otimes$ -- внешнее произведение, а $\hat S_{l}(t)$ -- активность слоя $l$, обработанная низкочастотным фильтром, аналогичным синаптическому \eqref{eq:p2_2_syn}, но с другой константой $\tau_{l}$.\\
\indent Для скрытых слоев можно применить логику аналогичную правилу обратного распространения ошибки \cite{wossermanNeuroComp}:
\begin{equation} \label{eq:p2_3_bp}
\delta u_{i}(t) = \frac{\partial C_{t}}{\partial u_{t}(t)} = (W_{i+1} \delta u_{i+1}) \odot F_{act}'(u_{i}(t))
\end{equation}
\begin{equation}
\Delta W_{i} = - \delta A_{i}(t) \otimes I_{i}(t),
\end{equation}
здесь $\odot$ означает поэлементное умножение.
\subsection{Регулируемая обратная связь}
\indent Не смотря на то, что на сегодняшний день правило обратного распространения ошибки до сих является наиболее популярным и наиболее эффективным методом обучения нейронных сетей, биологическая оправданность этого способа стоит под вопросом не менее давно \cite{rumelhart1986david, grossberg1987competitive, crick1989recent}.\\
\indent Серия работ на тематику регулируемой обратной связи (\textit{feedback alignment}) \cite{lillicrap2014random, nokland2016direct, liao2015important} открывает новые направления работ на тематику методов обучения равных по эффективности методу обратному распространению ошибки, но исключающие наиболее ненатуральный момент в нём -- необходимость обратного распространения информации в сети.\\
\indent Как видно из результатов \cite{lillicrap2014random} для работы правило обучения, достаточно заменить матрицу $W_{i+1}$ из \eqref{eq:p2_3_bp} на случайную матрицу $B_{i}$, при этом работа \cite{nokland2016direct} показала, что и второй множитель этой матрицы можно заменить, на уровень ошибки $C_{i}$, получая таким образом
\begin{equation} \label{eq:p2_3_fa}
\delta u_{i}(t) = \frac{\partial C_{t}}{\partial u_{t}(t)} = (B_{i} C_{i}(t)) \odot F_{act}'(u_{i}(t)).
\end{equation}
 
